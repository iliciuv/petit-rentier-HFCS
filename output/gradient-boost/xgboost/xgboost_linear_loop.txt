##### xgb.Booster
raw: 1.1 Mb 
call:
  xgb.train(params = params_clas, data = dtrain, nrounds = 500)
params (as set within xgb.train):
  booster = "gbtree", objective = "reg:squarederror", eval_metric = "rmse", eta = "0.1", max_depth = "6", validate_parameters = "TRUE"
xgb.attributes:
  niter
callbacks:
  cb.print.evaluation(period = print_every_n)
# of features: 8 
niter: 500
nfeatures : 8 
[1] "IMPORTANCE MATRIX:"
    Feature      Gain     Cover  Frequency
1: employer 0.6600930 0.2007673 0.04169794
2:  sv_year 0.2232993 0.5984658 0.86281378
3:     bage 0.1166078 0.2007669 0.09548828
[1] "Accuracy (Classification): 0.00011832919181162"
[1] "Test MSE:  57969063510836"
