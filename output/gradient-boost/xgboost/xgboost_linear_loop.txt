##### xgb.Booster
raw: 157.1 Kb 
call:
  xgb.train(params = params, data = dtrain, nrounds = nrounds, 
    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, 
    early_stopping_rounds = early_stopping_rounds, maximize = maximize, 
    save_period = save_period, save_name = save_name, xgb_model = xgb_model, 
    callbacks = callbacks)
params (as set within xgb.train):
  booster = "gbtree", objective = "reg:squarederror", eta = "0.1", gamma = "1", max_depth = "6", min_child_weight = "1", subsample = "0.5", colsample_bytree = "1", validate_parameters = "TRUE"
xgb.attributes:
  niter
callbacks:
  cb.print.evaluation(period = print_every_n)
  cb.evaluation.log()
# of features: 6 
niter: 100
nfeatures : 6 
evaluation_log:
    iter train_rmse
       1    1404448
       2    1401138
---                
      99    1386469
     100    1386469
[1] "IMPORTANCE MATRIX:"
   Feature      Gain     Cover Frequency
1: sv_year 0.7878094 0.7650431 0.7436096
2:    bage 0.2121906 0.2349569 0.2563904
[1] "Accuracy (Classification): 0"
[1] "Test MSE:  59400684818647.8"
